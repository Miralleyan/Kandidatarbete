{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install torchmin\n",
    "Installing torchmin with\n",
    "```\n",
    "pip install pytorch-minimize\n",
    "```\n",
    "Documentation for [torchmin](https://pytorch-minimize.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "# Using torchmin for constrained optimization\n",
    "```\n",
    "minimize_constr(f, x0[, constr, bounds, â€¦])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2638, grad_fn=<SelectBackward0>) tensor([0.2638, 0.4921], grad_fn=<PowBackward0>) tensor(0.1298, grad_fn=<MulBackward0>)\n",
      "tensor([0.2638, 0.4921], grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mprint\u001b[39m(pars0)\n\u001b[0;32m     54\u001b[0m \u001b[39m#print(pars0.square())\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m# res=minimize_constr(loss_function,pars0,constr=dict(fun=lambda x: x.sum(), lb=1, ub=1),bounds=dict(lb=0,ub=1),tol=1e-9,disp=1)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m res\u001b[39m=\u001b[39mminimize_constr(loss_function,pars0,constr\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(fun\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49msum(), lb\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, ub\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),bounds\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(lb\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,ub\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),tol\u001b[39m=\u001b[39;49m\u001b[39m1e-9\u001b[39;49m,disp\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m     59\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\torchmin\\minimize_constr.py:216\u001b[0m, in \u001b[0;36mminimize_constr\u001b[1;34m(f, x0, constr, bounds, max_iter, tol, callback, disp, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39m# optimize\u001b[39;00m\n\u001b[0;32m    215\u001b[0m x0_np \u001b[39m=\u001b[39m x0\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 216\u001b[0m result \u001b[39m=\u001b[39m minimize(\n\u001b[0;32m    217\u001b[0m     f_with_jac, x0_np, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrust-constr\u001b[39;49m\u001b[39m'\u001b[39;49m, jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    218\u001b[0m     hess\u001b[39m=\u001b[39;49mf_hess, callback\u001b[39m=\u001b[39;49mcallback, tol\u001b[39m=\u001b[39;49mtol,\n\u001b[0;32m    219\u001b[0m     bounds\u001b[39m=\u001b[39;49mbounds,\n\u001b[0;32m    220\u001b[0m     constraints\u001b[39m=\u001b[39;49mconstraints,\n\u001b[0;32m    221\u001b[0m     options\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(verbose\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(disp), maxiter\u001b[39m=\u001b[39;49mmax_iter, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    222\u001b[0m )\n\u001b[0;32m    224\u001b[0m \u001b[39m# convert the important things to torch tensors\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgrad\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\optimize\\_minimize.py:708\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    705\u001b[0m     res \u001b[39m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m    706\u001b[0m                           constraints, callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    707\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrust-constr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 708\u001b[0m     res \u001b[39m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    709\u001b[0m                                        bounds, constraints,\n\u001b[0;32m    710\u001b[0m                                        callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    711\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdogleg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    712\u001b[0m     res \u001b[39m=\u001b[39m _minimize_dogleg(fun, x0, args, jac, hess,\n\u001b[0;32m    713\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\minimize_trustregion_constr.py:340\u001b[0m, in \u001b[0;36m_minimize_trustregion_constr\u001b[1;34m(fun, x0, args, grad, hess, hessp, bounds, constraints, xtol, gtol, barrier_tol, sparse_jacobian, callback, maxiter, verbose, finite_diff_rel_step, initial_constr_penalty, initial_tr_radius, initial_barrier_parameter, initial_barrier_tolerance, factorization_method, disp)\u001b[0m\n\u001b[0;32m    337\u001b[0m     constraints \u001b[39m=\u001b[39m [constraints]\n\u001b[0;32m    339\u001b[0m \u001b[39m# Prepare constraints.\u001b[39;00m\n\u001b[1;32m--> 340\u001b[0m prepared_constraints \u001b[39m=\u001b[39m [\n\u001b[0;32m    341\u001b[0m     PreparedConstraint(c, x0, sparse_jacobian, finite_diff_bounds)\n\u001b[0;32m    342\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m constraints]\n\u001b[0;32m    344\u001b[0m \u001b[39m# Check that all constraints are either sparse or dense.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m n_sparse \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(c\u001b[39m.\u001b[39mfun\u001b[39m.\u001b[39msparse_jacobian \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m prepared_constraints)\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\minimize_trustregion_constr.py:341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    337\u001b[0m     constraints \u001b[39m=\u001b[39m [constraints]\n\u001b[0;32m    339\u001b[0m \u001b[39m# Prepare constraints.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m prepared_constraints \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 341\u001b[0m     PreparedConstraint(c, x0, sparse_jacobian, finite_diff_bounds)\n\u001b[0;32m    342\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m constraints]\n\u001b[0;32m    344\u001b[0m \u001b[39m# Check that all constraints are either sparse or dense.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m n_sparse \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(c\u001b[39m.\u001b[39mfun\u001b[39m.\u001b[39msparse_jacobian \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m prepared_constraints)\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\optimize\\_constraints.py:329\u001b[0m, in \u001b[0;36mPreparedConstraint.__init__\u001b[1;34m(self, constraint, x0, sparse_jacobian, finite_diff_bounds)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, constraint, x0, sparse_jacobian\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m              finite_diff_bounds\u001b[39m=\u001b[39m(\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)):\n\u001b[0;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(constraint, NonlinearConstraint):\n\u001b[1;32m--> 329\u001b[0m         fun \u001b[39m=\u001b[39m VectorFunction(constraint\u001b[39m.\u001b[39;49mfun, x0,\n\u001b[0;32m    330\u001b[0m                              constraint\u001b[39m.\u001b[39;49mjac, constraint\u001b[39m.\u001b[39;49mhess,\n\u001b[0;32m    331\u001b[0m                              constraint\u001b[39m.\u001b[39;49mfinite_diff_rel_step,\n\u001b[0;32m    332\u001b[0m                              constraint\u001b[39m.\u001b[39;49mfinite_diff_jac_sparsity,\n\u001b[0;32m    333\u001b[0m                              finite_diff_bounds, sparse_jacobian)\n\u001b[0;32m    334\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(constraint, LinearConstraint):\n\u001b[0;32m    335\u001b[0m         fun \u001b[39m=\u001b[39m LinearVectorFunction(constraint\u001b[39m.\u001b[39mA, x0, sparse_jacobian)\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:437\u001b[0m, in \u001b[0;36mVectorFunction.__init__\u001b[1;34m(self, fun, x0, jac, hess, finite_diff_rel_step, finite_diff_jac_sparsity, finite_diff_bounds, sparse_jacobian)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39m# Define Hessian\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39mif\u001b[39;00m callable(hess):\n\u001b[1;32m--> 437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH \u001b[39m=\u001b[39m hess(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv)\n\u001b[0;32m    438\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnhev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\torchmin\\minimize_constr.py:90\u001b[0m, in \u001b[0;36m_build_constr.<locals>.f_hess\u001b[1;34m(x, v)\u001b[0m\n\u001b[0;32m     88\u001b[0m     hvp, \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(grad, x, p, retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m v[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m hvp\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m LinearOperator((numel, numel), matvec\u001b[39m=\u001b[39;49mmatvec)\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:521\u001b[0m, in \u001b[0;36m_CustomLinearOperator.__init__\u001b[1;34m(self, shape, matvec, rmatvec, matmat, dtype, rmatmat)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__rmatmat_impl \u001b[39m=\u001b[39m rmatmat\n\u001b[0;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__matmat_impl \u001b[39m=\u001b[39m matmat\n\u001b[1;32m--> 521\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_dtype()\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:178\u001b[0m, in \u001b[0;36mLinearOperator._init_dtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     v \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m--> 178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatvec(v))\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:232\u001b[0m, in \u001b[0;36mLinearOperator.matvec\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (N,) \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (N,\u001b[39m1\u001b[39m):\n\u001b[0;32m    230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdimension mismatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_matvec(x)\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, np\u001b[39m.\u001b[39mmatrix):\n\u001b[0;32m    235\u001b[0m     y \u001b[39m=\u001b[39m asmatrix(y)\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:530\u001b[0m, in \u001b[0;36m_CustomLinearOperator._matvec\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_matvec\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__matvec_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\torchmin\\minimize_constr.py:88\u001b[0m, in \u001b[0;36m_build_constr.<locals>.f_hess.<locals>.matvec\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatvec\u001b[39m(p):\n\u001b[0;32m     87\u001b[0m     p \u001b[39m=\u001b[39m to_tensor(p)\n\u001b[1;32m---> 88\u001b[0m     hvp, \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(grad, x, p, retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m v[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m hvp\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\filip\\miniconda3\\envs\\local\\lib\\site-packages\\torch\\autograd\\__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[0;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from torchmin import minimize_constr\n",
    "\n",
    "torch.manual_seed(382)\n",
    "pars0 = torch.randn(2, requires_grad=True)\n",
    "pars0 = pars0.square()\n",
    "#pars0 = torch.Variable(pars0, requires_grad=True)\n",
    "pars = torch.nn.Parameter(pars0, requires_grad =True)\n",
    "# pars  = pars0\n",
    "\n",
    "opt = torch.optim.SGD(params=[pars], lr=0.1)\n",
    "\n",
    "\n",
    "def loss_function(pars):\n",
    "    return pars[0]*pars[1]\n",
    "\n",
    "print(pars[0], pars0, loss_function(pars0))\n",
    "# res = minimize_constr(\n",
    "#     loss_function, pars0, \n",
    "#     max_iter=100,\n",
    "#     constr=dict(\n",
    "#         fun=lambda x: x[0]+x[1], \n",
    "#         lb=1, ub=1\n",
    "#     ),\n",
    "#     bounds=dict(\n",
    "#         lb=0, ub=1\n",
    "#     ),\n",
    "#     disp=1\n",
    "# )\n",
    "\n",
    "def loss_function2(p):\n",
    "    return p*(p-1)\n",
    "# p0 = torch.randn(1, requires_grad=True)\n",
    "# p = torch.nn.Parameter(p0, requires_grad =True)\n",
    "# res = minimize_constr(\n",
    "#     loss_function2, p, \n",
    "#     max_iter=100,\n",
    "#     constr=dict(\n",
    "#         fun=lambda x: x, \n",
    "#         lb=1, ub=1\n",
    "#     ),\n",
    "#     disp=1\n",
    "# )\n",
    "x0=torch.tensor(0.6)\n",
    "def constr1(pars):\n",
    "    return pars.sum()\n",
    "# res=minimize_constr(loss_function2, x0,constr=dict(fun=constr1, lb=1, ub=1),bounds=dict(lb=0,ub=1),tol=1e-9,disp=1)\n",
    "#res=minimize_constr(loss_function,pars0,constr=dict(fun=constr1, lb=1, ub=1),bounds=dict(lb=0,ub=1),tol=1e-9,disp=1)\n",
    "print(pars0)\n",
    "#print(pars0.square())\n",
    "# res=minimize_constr(loss_function,pars0,constr=dict(fun=lambda x: x.sum(), lb=1, ub=1),bounds=dict(lb=0,ub=1),tol=1e-9,disp=1)\n",
    "res=minimize_constr(loss_function,pars0,constr=dict(fun=lambda x: x.sum(), lb=1, ub=1),bounds=dict(lb=0,ub=1),tol=1e-9,disp=1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    opt.zero_grad()\n",
    "    loss = loss_function(pars)\n",
    "    loss.backward()\n",
    "    print(pars.grad)\n",
    "    opt.step()\n",
    "    print(pars)\n",
    "    print(res.x,res.fun)\n",
    "    pq = [par.item() for par in pars[:2]]\n",
    "    plt.plot(pq[0], pq[1], \".\")\n",
    "\n",
    "# pars = pars.detach()\n",
    "# pars[7:].clamp_(0)\n",
    "print(pars)\n",
    "\n",
    "plt.plot(np.linspace(0,1), np.ones(50)-np.linspace(0,1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class constraint_sum_to_1(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bfab563a781f4da718747e2fae872b9d7cb73fc061a7ef5b3d06d4d87105164"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
