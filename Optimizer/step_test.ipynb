{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_measure as pm\n",
    "import numpy as np\n",
    "N = 17 # number of weights\n",
    "M = 1000 # Number of datapoints\n",
    "verbose = True\n",
    "dev = 'cpu'\n",
    "\n",
    "def regression_model(a,x):\n",
    "    return a+x\n",
    "\n",
    "x = torch.linspace(0, 10, M)\n",
    "data = regression_model(torch.randn(M).to(dev) - 2, x)\n",
    "\n",
    "w = torch.ones(N,dtype=torch.float).to(dev)\n",
    "w = torch.nn.parameter.Parameter(w/w.sum())\n",
    "l = torch.linspace(-6, 2, N, requires_grad=False).to(dev)\n",
    "\n",
    "index = []\n",
    "for i in range(M):\n",
    "    ab = (regression_model(l, x[i]) - data[i]).abs()\n",
    "    index.append(torch.argmin(ab))\n",
    "\n",
    "def NLLLoss(m:list[pm.Measure]):\n",
    "    return -m[0].weights[index].log().sum()\n",
    "\n",
    "sd = (l[index] - data)**2\n",
    "def WardLoss(w):\n",
    "    return sum(sd * w[index])\n",
    "\n",
    "def K(d):\n",
    "        return 1/np.sqrt(2*np.pi)*np.exp(-d**2/2)\n",
    "h=1.06*M**(-1/5)\n",
    "kde_mat = K((regression_model(l, x.view(-1,1)) - data.view(-1,1)) / h)\n",
    "\n",
    "#a = torch.sum(kde_mat, dim=0)\n",
    "\n",
    "def KDENLLLoss(m):\n",
    "    return -(torch.matmul(kde_mat ,m[0].weights.view(-1,1))/(M*h)).log().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0          Loss: 9044.3340  LR: 0.001000000\n",
      "Epoch: 1          Loss: 9040.9580  LR: 0.001000000\n",
      "Epoch: 2          Loss: 9037.6104  LR: 0.001000000\n",
      "Epoch: 3          Loss: 9034.2939  LR: 0.001000000\n",
      "Epoch: 4          Loss: 9031.0049  LR: 0.001000000\n",
      "Epoch: 5          Loss: 9027.7461  LR: 0.001000000\n",
      "Epoch: 6          Loss: 9024.5137  LR: 0.001000000\n",
      "Epoch: 7          Loss: 9021.3096  LR: 0.001000000\n",
      "Epoch: 8          Loss: 9018.1328  LR: 0.001000000\n",
      "Epoch: 9          Loss: 9014.9814  LR: 0.001000000\n",
      "Epoch: 10         Loss: 9011.8555  LR: 0.001000000\n",
      "Epoch: 11         Loss: 9008.7568  LR: 0.001000000\n",
      "Epoch: 12         Loss: 9005.6816  LR: 0.001000000\n",
      "Epoch: 13         Loss: 9002.6309  LR: 0.001000000\n",
      "Epoch: 14         Loss: 8999.6055  LR: 0.001000000\n",
      "Epoch: 15         Loss: 8996.6016  LR: 0.001000000\n",
      "Epoch: 16         Loss: 8993.6230  LR: 0.001000000\n",
      "Epoch: 17         Loss: 8990.6670  LR: 0.001000000\n",
      "Epoch: 18         Loss: 8987.7324  LR: 0.001000000\n",
      "Epoch: 19         Loss: 8984.8203  LR: 0.001000000\n",
      "Epoch: 20         Loss: 8981.9297  LR: 0.001000000\n",
      "Epoch: 21         Loss: 8979.0605  LR: 0.001000000\n",
      "Epoch: 22         Loss: 8976.2129  LR: 0.001000000\n",
      "Epoch: 23         Loss: 8973.3857  LR: 0.001000000\n",
      "Epoch: 24         Loss: 8970.5791  LR: 0.001000000\n",
      "Epoch: 25         Loss: 8967.7910  LR: 0.001000000\n",
      "Epoch: 26         Loss: 8965.0234  LR: 0.001000000\n",
      "Epoch: 27         Loss: 8962.2764  LR: 0.001000000\n",
      "Epoch: 28         Loss: 8959.5479  LR: 0.001000000\n",
      "Epoch: 29         Loss: 8956.8359  LR: 0.001000000\n",
      "Epoch: 30         Loss: 8954.1445  LR: 0.001000000\n",
      "Epoch: 31         Loss: 8951.4717  LR: 0.001000000\n",
      "Epoch: 32         Loss: 8948.8154  LR: 0.001000000\n",
      "Epoch: 33         Loss: 8946.1777  LR: 0.001000000\n",
      "Epoch: 34         Loss: 8943.5586  LR: 0.001000000\n",
      "Epoch: 35         Loss: 8940.9531  LR: 0.001000000\n",
      "Epoch: 36         Loss: 8938.3672  LR: 0.001000000\n",
      "Epoch: 37         Loss: 8935.7979  LR: 0.001000000\n",
      "Epoch: 38         Loss: 8933.2432  LR: 0.001000000\n",
      "Epoch: 39         Loss: 8930.7070  LR: 0.001000000\n",
      "Epoch: 40         Loss: 8928.1846  LR: 0.001000000\n",
      "Epoch: 41         Loss: 8925.6797  LR: 0.001000000\n",
      "Epoch: 42         Loss: 8923.1895  LR: 0.001000000\n",
      "Epoch: 43         Loss: 8920.7148  LR: 0.001000000\n",
      "Epoch: 44         Loss: 8918.2549  LR: 0.001000000\n",
      "Epoch: 45         Loss: 8915.8105  LR: 0.001000000\n",
      "Epoch: 46         Loss: 8913.3809  LR: 0.001000000\n",
      "Epoch: 47         Loss: 8910.9658  LR: 0.001000000\n",
      "Epoch: 48         Loss: 8908.5635  LR: 0.001000000\n",
      "Epoch: 49         Loss: 8906.1768  LR: 0.001000000\n",
      "Epoch: 50         Loss: 8903.8027  LR: 0.001000000\n",
      "Epoch: 51         Loss: 8901.4355  LR: 0.001000000\n",
      "Epoch: 52         Loss: 8899.0840  LR: 0.001000000\n",
      "Epoch: 53         Loss: 8896.7412  LR: 0.001000000\n",
      "Epoch: 54         Loss: 8894.4082  LR: 0.001000000\n",
      "Epoch: 55         Loss: 8892.0898  LR: 0.001000000\n",
      "Epoch: 56         Loss: 8889.7744  LR: 0.001000000\n",
      "Epoch: 57         Loss: 8887.4688  LR: 0.001000000\n",
      "Epoch: 58         Loss: 8885.1758  LR: 0.001000000\n",
      "Epoch: 59         Loss: 8882.8857  LR: 0.001000000\n",
      "Epoch: 60         Loss: 8880.6064  LR: 0.001000000\n",
      "Epoch: 61         Loss: 8878.3369  LR: 0.001000000\n",
      "Epoch: 62         Loss: 8876.0723  LR: 0.001000000\n",
      "Epoch: 63         Loss: 8873.8184  LR: 0.001000000\n",
      "Epoch: 64         Loss: 8871.5723  LR: 0.001000000\n",
      "Epoch: 65         Loss: 8869.3301  LR: 0.001000000\n",
      "Epoch: 66         Loss: 8867.1025  LR: 0.001000000\n",
      "Epoch: 67         Loss: 8864.8779  LR: 0.001000000\n",
      "Epoch: 68         Loss: 8862.6592  LR: 0.001000000\n",
      "Epoch: 69         Loss: 8860.4531  LR: 0.001000000\n",
      "Epoch: 70         Loss: 8858.2520  LR: 0.001000000\n",
      "Epoch: 71         Loss: 8856.0576  LR: 0.001000000\n",
      "Epoch: 72         Loss: 8853.8730  LR: 0.001000000\n",
      "Epoch: 73         Loss: 8851.6934  LR: 0.001000000\n",
      "Epoch: 74         Loss: 8849.5234  LR: 0.001000000\n",
      "Epoch: 75         Loss: 8847.3604  LR: 0.001000000\n",
      "Epoch: 76         Loss: 8845.2031  LR: 0.001000000\n",
      "Epoch: 77         Loss: 8843.0557  LR: 0.001000000\n",
      "Epoch: 78         Loss: 8840.9141  LR: 0.001000000\n",
      "Epoch: 79         Loss: 8838.7764  LR: 0.001000000\n",
      "Epoch: 80         Loss: 8836.6504  LR: 0.001000000\n",
      "Epoch: 81         Loss: 8834.5293  LR: 0.001000000\n",
      "Epoch: 82         Loss: 8832.4141  LR: 0.001000000\n",
      "Epoch: 83         Loss: 8830.3066  LR: 0.001000000\n",
      "Epoch: 84         Loss: 8828.2080  LR: 0.001000000\n",
      "Epoch: 85         Loss: 8826.1133  LR: 0.001000000\n",
      "Epoch: 86         Loss: 8824.0264  LR: 0.001000000\n",
      "Epoch: 87         Loss: 8821.9473  LR: 0.001000000\n",
      "Epoch: 88         Loss: 8819.8750  LR: 0.001000000\n",
      "Epoch: 89         Loss: 8817.8076  LR: 0.001000000\n",
      "Epoch: 90         Loss: 8815.7480  LR: 0.001000000\n",
      "Epoch: 91         Loss: 8813.6943  LR: 0.001000000\n",
      "Epoch: 92         Loss: 8811.6465  LR: 0.001000000\n",
      "Epoch: 93         Loss: 8809.6074  LR: 0.001000000\n",
      "Epoch: 94         Loss: 8807.5723  LR: 0.001000000\n",
      "Epoch: 95         Loss: 8805.5469  LR: 0.001000000\n",
      "Epoch: 96         Loss: 8803.5234  LR: 0.001000000\n",
      "Epoch: 97         Loss: 8801.5078  LR: 0.001000000\n",
      "Epoch: 98         Loss: 8799.5000  LR: 0.001000000\n",
      "Epoch: 99         Loss: 8797.4980  LR: 0.001000000\n",
      "Epoch: 100        Loss: 8795.4990  LR: 0.001000000\n",
      "Epoch: 101        Loss: 8793.5107  LR: 0.001000000\n",
      "Epoch: 102        Loss: 8791.5254  LR: 0.001000000\n",
      "Epoch: 103        Loss: 8789.5469  LR: 0.001000000\n",
      "Epoch: 104        Loss: 8787.5762  LR: 0.001000000\n",
      "Epoch: 105        Loss: 8785.6084  LR: 0.001000000\n",
      "Epoch: 106        Loss: 8783.6484  LR: 0.001000000\n",
      "Epoch: 107        Loss: 8781.6953  LR: 0.001000000\n",
      "Epoch: 108        Loss: 8779.7441  LR: 0.001000000\n",
      "Epoch: 109        Loss: 8777.8047  LR: 0.001000000\n",
      "Epoch: 110        Loss: 8775.8662  LR: 0.001000000\n",
      "Epoch: 111        Loss: 8773.9336  LR: 0.001000000\n",
      "Epoch: 112        Loss: 8772.0098  LR: 0.001000000\n",
      "Epoch: 113        Loss: 8770.0889  LR: 0.001000000\n",
      "Epoch: 114        Loss: 8768.1738  LR: 0.001000000\n",
      "Epoch: 115        Loss: 8766.2676  LR: 0.001000000\n",
      "Epoch: 116        Loss: 8764.3633  LR: 0.001000000\n",
      "Epoch: 117        Loss: 8762.4688  LR: 0.001000000\n",
      "Epoch: 118        Loss: 8760.5859  LR: 0.001000000\n",
      "Epoch: 119        Loss: 8758.7061  LR: 0.001000000\n",
      "Epoch: 120        Loss: 8756.8350  LR: 0.001000000\n",
      "Epoch: 121        Loss: 8754.9707  LR: 0.001000000\n",
      "Epoch: 122        Loss: 8753.1055  LR: 0.001000000\n",
      "Epoch: 123        Loss: 8751.2529  LR: 0.001000000\n",
      "Epoch: 124        Loss: 8749.4004  LR: 0.001000000\n",
      "Epoch: 125        Loss: 8747.5547  LR: 0.001000000\n",
      "Epoch: 126        Loss: 8745.7168  LR: 0.001000000\n",
      "Epoch: 127        Loss: 8743.8809  LR: 0.001000000\n",
      "Epoch: 128        Loss: 8742.0508  LR: 0.001000000\n",
      "Epoch: 129        Loss: 8740.2295  LR: 0.001000000\n",
      "Epoch: 130        Loss: 8738.4082  LR: 0.001000000\n",
      "Epoch: 131        Loss: 8736.5957  LR: 0.001000000\n",
      "Epoch: 132        Loss: 8734.7891  LR: 0.001000000\n",
      "Epoch: 133        Loss: 8732.9824  LR: 0.001000000\n",
      "Epoch: 134        Loss: 8731.1865  LR: 0.001000000\n",
      "Epoch: 135        Loss: 8729.3936  LR: 0.001000000\n",
      "Epoch: 136        Loss: 8727.6055  LR: 0.001000000\n",
      "Epoch: 137        Loss: 8725.8242  LR: 0.001000000\n",
      "Epoch: 138        Loss: 8724.0449  LR: 0.001000000\n",
      "Epoch: 139        Loss: 8722.2725  LR: 0.001000000\n",
      "Epoch: 140        Loss: 8720.5059  LR: 0.001000000\n",
      "Epoch: 141        Loss: 8718.7422  LR: 0.001000000\n",
      "Epoch: 142        Loss: 8716.9863  LR: 0.001000000\n",
      "Epoch: 143        Loss: 8715.2324  LR: 0.001000000\n",
      "Epoch: 144        Loss: 8713.4834  LR: 0.001000000\n",
      "Epoch: 145        Loss: 8711.7412  LR: 0.001000000\n",
      "Epoch: 146        Loss: 8710.0000  LR: 0.001000000\n",
      "Epoch: 147        Loss: 8708.2646  LR: 0.001000000\n",
      "Epoch: 148        Loss: 8706.5312  LR: 0.001000000\n",
      "Epoch: 149        Loss: 8704.8076  LR: 0.001000000\n",
      "Epoch: 150        Loss: 8703.0859  LR: 0.001000000\n",
      "Epoch: 151        Loss: 8701.3652  LR: 0.001000000\n",
      "Epoch: 152        Loss: 8699.6514  LR: 0.001000000\n",
      "Epoch: 153        Loss: 8697.9414  LR: 0.001000000\n",
      "Epoch: 154        Loss: 8696.2354  LR: 0.001000000\n",
      "Epoch: 155        Loss: 8694.5352  LR: 0.001000000\n",
      "Epoch: 156        Loss: 8692.8379  LR: 0.001000000\n",
      "Epoch: 157        Loss: 8691.1436  LR: 0.001000000\n",
      "Epoch: 158        Loss: 8689.4531  LR: 0.001000000\n",
      "Epoch: 159        Loss: 8687.7705  LR: 0.001000000\n",
      "Epoch: 160        Loss: 8686.0908  LR: 0.001000000\n",
      "Epoch: 161        Loss: 8684.4121  LR: 0.001000000\n",
      "Epoch: 162        Loss: 8682.7393  LR: 0.001000000\n",
      "Epoch: 163        Loss: 8681.0713  LR: 0.001000000\n",
      "Epoch: 164        Loss: 8679.4062  LR: 0.001000000\n",
      "Epoch: 165        Loss: 8677.7480  LR: 0.001000000\n",
      "Epoch: 166        Loss: 8676.0918  LR: 0.001000000\n",
      "Epoch: 167        Loss: 8674.4395  LR: 0.001000000\n",
      "Epoch: 168        Loss: 8672.7891  LR: 0.001000000\n",
      "Epoch: 169        Loss: 8671.1475  LR: 0.001000000\n",
      "Epoch: 170        Loss: 8669.5088  LR: 0.001000000\n",
      "Epoch: 171        Loss: 8667.8701  LR: 0.001000000\n",
      "Epoch: 172        Loss: 8666.2402  LR: 0.001000000\n",
      "Epoch: 173        Loss: 8664.6113  LR: 0.001000000\n",
      "Epoch: 174        Loss: 8662.9893  LR: 0.001000000\n",
      "Epoch: 175        Loss: 8661.3701  LR: 0.001000000\n",
      "Epoch: 176        Loss: 8659.7568  LR: 0.001000000\n",
      "Epoch: 177        Loss: 8658.1445  LR: 0.001000000\n",
      "Epoch: 178        Loss: 8656.5391  LR: 0.001000000\n",
      "Epoch: 179        Loss: 8654.9326  LR: 0.001000000\n",
      "Epoch: 180        Loss: 8653.3340  LR: 0.001000000\n",
      "Epoch: 181        Loss: 8651.7354  LR: 0.001000000\n",
      "Epoch: 182        Loss: 8650.1426  LR: 0.001000000\n",
      "Epoch: 183        Loss: 8648.5518  LR: 0.001000000\n",
      "Epoch: 184        Loss: 8646.9668  LR: 0.001000000\n",
      "Epoch: 185        Loss: 8645.3828  LR: 0.001000000\n",
      "Epoch: 186        Loss: 8643.8037  LR: 0.001000000\n",
      "Epoch: 187        Loss: 8642.2275  LR: 0.001000000\n",
      "Epoch: 188        Loss: 8640.6582  LR: 0.001000000\n",
      "Epoch: 189        Loss: 8639.0889  LR: 0.001000000\n",
      "Epoch: 190        Loss: 8637.5234  LR: 0.001000000\n",
      "Epoch: 191        Loss: 8635.9600  LR: 0.001000000\n",
      "Epoch: 192        Loss: 8634.4023  LR: 0.001000000\n",
      "Epoch: 193        Loss: 8632.8477  LR: 0.001000000\n",
      "Epoch: 194        Loss: 8631.2988  LR: 0.001000000\n",
      "Epoch: 195        Loss: 8629.7500  LR: 0.001000000\n",
      "Epoch: 196        Loss: 8628.2070  LR: 0.001000000\n",
      "Epoch: 197        Loss: 8626.6680  LR: 0.001000000\n",
      "Epoch: 198        Loss: 8625.1279  LR: 0.001000000\n",
      "Epoch: 199        Loss: 8623.5967  LR: 0.001000000\n",
      "Epoch: 200        Loss: 8622.0703  LR: 0.001000000\n",
      "Epoch: 201        Loss: 8620.5439  LR: 0.001000000\n",
      "Epoch: 202        Loss: 8619.0244  LR: 0.001000000\n",
      "Epoch: 203        Loss: 8617.5049  LR: 0.001000000\n",
      "Epoch: 204        Loss: 8615.9902  LR: 0.001000000\n",
      "Epoch: 205        Loss: 8614.4805  LR: 0.001000000\n",
      "Epoch: 206        Loss: 8612.9756  LR: 0.001000000\n",
      "Epoch: 207        Loss: 8611.4727  LR: 0.001000000\n",
      "Epoch: 208        Loss: 8609.9697  LR: 0.001000000\n",
      "Epoch: 209        Loss: 8608.4707  LR: 0.001000000\n",
      "Epoch: 210        Loss: 8606.9756  LR: 0.001000000\n",
      "Epoch: 211        Loss: 8605.4883  LR: 0.001000000\n",
      "Epoch: 212        Loss: 8603.9980  LR: 0.001000000\n",
      "Epoch: 213        Loss: 8602.5117  LR: 0.001000000\n",
      "Epoch: 214        Loss: 8601.0293  LR: 0.001000000\n",
      "Epoch: 215        Loss: 8599.5479  LR: 0.001000000\n",
      "Epoch: 216        Loss: 8598.0723  LR: 0.001000000\n",
      "Epoch: 217        Loss: 8596.6016  LR: 0.001000000\n",
      "Epoch: 218        Loss: 8595.1318  LR: 0.001000000\n",
      "Epoch: 219        Loss: 8593.6621  LR: 0.001000000\n",
      "Epoch: 220        Loss: 8592.1982  LR: 0.001000000\n",
      "Epoch: 221        Loss: 8590.7363  LR: 0.001000000\n",
      "Epoch: 222        Loss: 8589.2783  LR: 0.001000000\n",
      "Epoch: 223        Loss: 8587.8262  LR: 0.001000000\n",
      "Epoch: 224        Loss: 8586.3730  LR: 0.001000000\n",
      "Epoch: 225        Loss: 8584.9219  LR: 0.001000000\n",
      "Epoch: 226        Loss: 8583.4756  LR: 0.001000000\n",
      "Epoch: 227        Loss: 8582.0322  LR: 0.001000000\n",
      "Epoch: 228        Loss: 8580.5957  LR: 0.001000000\n",
      "Epoch: 229        Loss: 8579.1572  LR: 0.001000000\n",
      "Epoch: 230        Loss: 8577.7227  LR: 0.001000000\n",
      "Epoch: 231        Loss: 8576.2900  LR: 0.001000000\n",
      "Epoch: 232        Loss: 8574.8633  LR: 0.001000000\n",
      "Epoch: 233        Loss: 8573.4375  LR: 0.001000000\n",
      "Epoch: 234        Loss: 8572.0176  LR: 0.001000000\n",
      "Epoch: 235        Loss: 8570.5986  LR: 0.001000000\n",
      "Epoch: 236        Loss: 8569.1797  LR: 0.001000000\n",
      "Epoch: 237        Loss: 8567.7676  LR: 0.001000000\n",
      "Epoch: 238        Loss: 8566.3564  LR: 0.001000000\n",
      "Epoch: 239        Loss: 8564.9492  LR: 0.001000000\n",
      "Epoch: 240        Loss: 8563.5449  LR: 0.001000000\n",
      "Epoch: 241        Loss: 8562.1455  LR: 0.001000000\n",
      "Epoch: 242        Loss: 8560.7451  LR: 0.001000000\n",
      "Epoch: 243        Loss: 8559.3496  LR: 0.001000000\n",
      "Epoch: 244        Loss: 8557.9580  LR: 0.001000000\n",
      "Epoch: 245        Loss: 8556.5703  LR: 0.001000000\n",
      "Epoch: 246        Loss: 8555.1816  LR: 0.001000000\n",
      "Epoch: 247        Loss: 8553.7988  LR: 0.001000000\n",
      "Epoch: 248        Loss: 8552.4170  LR: 0.001000000\n",
      "Epoch: 249        Loss: 8551.0410  LR: 0.001000000\n",
      "Epoch: 250        Loss: 8549.6650  LR: 0.001000000\n",
      "Epoch: 251        Loss: 8548.2969  LR: 0.001000000\n",
      "Epoch: 252        Loss: 8546.9268  LR: 0.001000000\n",
      "Epoch: 253        Loss: 8545.5605  LR: 0.001000000\n",
      "Epoch: 254        Loss: 8544.1973  LR: 0.001000000\n",
      "Epoch: 255        Loss: 8542.8389  LR: 0.001000000\n",
      "Epoch: 256        Loss: 8541.4824  LR: 0.001000000\n",
      "Epoch: 257        Loss: 8540.1309  LR: 0.001000000\n",
      "Epoch: 258        Loss: 8538.7793  LR: 0.001000000\n",
      "Epoch: 259        Loss: 8537.4316  LR: 0.001000000\n",
      "Epoch: 260        Loss: 8536.0879  LR: 0.001000000\n",
      "Epoch: 261        Loss: 8534.7461  LR: 0.001000000\n",
      "Epoch: 262        Loss: 8533.4121  LR: 0.001000000\n",
      "Epoch: 263        Loss: 8532.0752  LR: 0.001000000\n",
      "Epoch: 264        Loss: 8530.7422  LR: 0.001000000\n",
      "Epoch: 265        Loss: 8529.4150  LR: 0.001000000\n",
      "Epoch: 266        Loss: 8528.0898  LR: 0.001000000\n",
      "Epoch: 267        Loss: 8526.7656  LR: 0.001000000\n",
      "Epoch: 268        Loss: 8525.4492  LR: 0.001000000\n",
      "Epoch: 269        Loss: 8524.1348  LR: 0.001000000\n",
      "Epoch: 270        Loss: 8522.8203  LR: 0.001000000\n",
      "Epoch: 271        Loss: 8521.5117  LR: 0.001000000\n",
      "Epoch: 272        Loss: 8520.2051  LR: 0.001000000\n",
      "Epoch: 273        Loss: 8518.9023  LR: 0.001000000\n",
      "Epoch: 274        Loss: 8517.6055  LR: 0.001000000\n",
      "Epoch: 275        Loss: 8516.3086  LR: 0.001000000\n",
      "Epoch: 276        Loss: 8515.0166  LR: 0.001000000\n",
      "Epoch: 277        Loss: 8513.7295  LR: 0.001000000\n",
      "Epoch: 278        Loss: 8512.4414  LR: 0.001000000\n",
      "Epoch: 279        Loss: 8511.1602  LR: 0.001000000\n",
      "Epoch: 280        Loss: 8509.8838  LR: 0.001000000\n",
      "Epoch: 281        Loss: 8508.6113  LR: 0.001000000\n",
      "Epoch: 282        Loss: 8507.3389  LR: 0.001000000\n",
      "Epoch: 283        Loss: 8506.0703  LR: 0.001000000\n",
      "Epoch: 284        Loss: 8504.8047  LR: 0.001000000\n",
      "Epoch: 285        Loss: 8503.5449  LR: 0.001000000\n",
      "Epoch: 286        Loss: 8502.2852  LR: 0.001000000\n",
      "Epoch: 287        Loss: 8501.0273  LR: 0.001000000\n",
      "Epoch: 288        Loss: 8499.7734  LR: 0.001000000\n",
      "Epoch: 289        Loss: 8498.5254  LR: 0.001000000\n",
      "Epoch: 290        Loss: 8497.2764  LR: 0.001000000\n",
      "Epoch: 291        Loss: 8496.0332  LR: 0.001000000\n",
      "Epoch: 292        Loss: 8494.7920  LR: 0.001000000\n",
      "Epoch: 293        Loss: 8493.5508  LR: 0.001000000\n",
      "Epoch: 294        Loss: 8492.3135  LR: 0.001000000\n",
      "Epoch: 295        Loss: 8491.0811  LR: 0.001000000\n",
      "Epoch: 296        Loss: 8489.8506  LR: 0.001000000\n",
      "Epoch: 297        Loss: 8488.6240  LR: 0.001000000\n",
      "Epoch: 298        Loss: 8487.4004  LR: 0.001000000\n",
      "Epoch: 299        Loss: 8486.1777  LR: 0.001000000\n",
      "Epoch: 300        Loss: 8484.9590  LR: 0.001000000\n",
      "Epoch: 301        Loss: 8483.7441  LR: 0.001000000\n",
      "Epoch: 302        Loss: 8482.5381  LR: 0.001000000\n",
      "Epoch: 303        Loss: 8481.3281  LR: 0.001000000\n",
      "Epoch: 304        Loss: 8480.1211  LR: 0.001000000\n",
      "Epoch: 305        Loss: 8478.9180  LR: 0.001000000\n",
      "Epoch: 306        Loss: 8477.7178  LR: 0.001000000\n",
      "Epoch: 307        Loss: 8476.5215  LR: 0.001000000\n",
      "Epoch: 308        Loss: 8475.3301  LR: 0.001000000\n",
      "Epoch: 309        Loss: 8474.1406  LR: 0.001000000\n",
      "Epoch: 310        Loss: 8472.9521  LR: 0.001000000\n",
      "Epoch: 311        Loss: 8471.7686  LR: 0.001000000\n",
      "Epoch: 312        Loss: 8470.5898  LR: 0.001000000\n",
      "Epoch: 313        Loss: 8469.4160  LR: 0.001000000\n",
      "Epoch: 314        Loss: 8468.2461  LR: 0.001000000\n",
      "Epoch: 315        Loss: 8467.0752  LR: 0.001000000\n",
      "Epoch: 316        Loss: 8465.9082  LR: 0.001000000\n",
      "Epoch: 317        Loss: 8464.7471  LR: 0.001000000\n",
      "Epoch: 318        Loss: 8463.5889  LR: 0.001000000\n",
      "Epoch: 319        Loss: 8462.4346  LR: 0.001000000\n",
      "Epoch: 320        Loss: 8461.2812  LR: 0.001000000\n",
      "Epoch: 321        Loss: 8460.1357  LR: 0.001000000\n",
      "Epoch: 322        Loss: 8458.9912  LR: 0.001000000\n",
      "Epoch: 323        Loss: 8457.8545  LR: 0.001000000\n",
      "Epoch: 324        Loss: 8456.7246  LR: 0.001000000\n",
      "Epoch: 325        Loss: 8455.5938  LR: 0.001000000\n",
      "Epoch: 326        Loss: 8454.4678  LR: 0.001000000\n",
      "Epoch: 327        Loss: 8453.3447  LR: 0.001000000\n",
      "Epoch: 328        Loss: 8452.2266  LR: 0.001000000\n",
      "Epoch: 329        Loss: 8451.1152  LR: 0.001000000\n",
      "Epoch: 330        Loss: 8450.0078  LR: 0.001000000\n",
      "Epoch: 331        Loss: 8448.9062  LR: 0.001000000\n",
      "Epoch: 332        Loss: 8447.8105  LR: 0.001000000\n",
      "Epoch: 333        Loss: 8446.7275  LR: 0.001000000\n",
      "Epoch: 334        Loss: 8445.6426  LR: 0.001000000\n",
      "Epoch: 335        Loss: 8444.5615  LR: 0.001000000\n",
      "Epoch: 336        Loss: 8443.4902  LR: 0.001000000\n",
      "Epoch: 337        Loss: 8442.4238  LR: 0.001000000\n",
      "Epoch: 338        Loss: 8441.3682  LR: 0.001000000\n",
      "Epoch: 339        Loss: 8440.3203  LR: 0.001000000\n",
      "Epoch: 340        Loss: 8439.2891  LR: 0.001000000\n",
      "Epoch: 341        Loss: 8438.2559  LR: 0.001000000\n",
      "Epoch: 342        Loss: 8437.2363  LR: 0.001000000\n",
      "Epoch: 343        Loss: 8436.2275  LR: 0.001000000\n",
      "Epoch: 344        Loss: 8435.2188  LR: 0.001000000\n",
      "Epoch: 345        Loss: 8434.2158  LR: 0.001000000\n",
      "Epoch: 346        Loss: 8433.2148  LR: 0.001000000\n",
      "Epoch: 347        Loss: 8432.2207  LR: 0.001000000\n",
      "Epoch: 348        Loss: 8431.2344  LR: 0.001000000\n",
      "Epoch: 349        Loss: 8430.2461  LR: 0.001000000\n",
      "Epoch: 350        Loss: 8429.2598  LR: 0.001000000\n",
      "Epoch: 351        Loss: 8428.2812  LR: 0.001000000\n",
      "Epoch: 352        Loss: 8427.3047  LR: 0.001000000\n",
      "Epoch: 353        Loss: 8426.3359  LR: 0.001000000\n",
      "Epoch: 354        Loss: 8425.3877  LR: 0.001000000\n",
      "Epoch: 355        Loss: 8424.4238  LR: 0.001000000\n",
      "Epoch: 356        Loss: 8423.4639  LR: 0.001000000\n",
      "Epoch: 357        Loss: 8422.5107  LR: 0.001000000\n",
      "Epoch: 358        Loss: 8421.5674  LR: 0.001000000\n",
      "Epoch: 359        Loss: 8420.6201  LR: 0.001000000\n",
      "Epoch: 360        Loss: 8419.6787  LR: 0.001000000\n",
      "Epoch: 361        Loss: 8418.7412  LR: 0.001000000\n",
      "Epoch: 362        Loss: 8417.8096  LR: 0.001000000\n",
      "Epoch: 363        Loss: 8416.8857  LR: 0.001000000\n",
      "Epoch: 364        Loss: 8415.9727  LR: 0.001000000\n",
      "Epoch: 365        Loss: 8415.0566  LR: 0.001000000\n",
      "Epoch: 366        Loss: 8414.1426  LR: 0.001000000\n",
      "Epoch: 367        Loss: 8413.2354  LR: 0.001000000\n",
      "Epoch: 368        Loss: 8412.3369  LR: 0.001000000\n",
      "Epoch: 369        Loss: 8411.4443  LR: 0.001000000\n",
      "Epoch: 370        Loss: 8410.5566  LR: 0.001000000\n",
      "Epoch: 371        Loss: 8409.6777  LR: 0.001000000\n",
      "Epoch: 372        Loss: 8408.8008  LR: 0.001000000\n",
      "Epoch: 373        Loss: 8407.9307  LR: 0.001000000\n",
      "Epoch: 374        Loss: 8407.0713  LR: 0.001000000\n",
      "Epoch: 375        Loss: 8406.2168  LR: 0.001000000\n",
      "Epoch: 376        Loss: 8405.3965  LR: 0.001000000\n",
      "Epoch: 377        Loss: 8404.5518  LR: 0.001000000\n",
      "Epoch: 378        Loss: 8403.7158  LR: 0.001000000\n",
      "Epoch: 379        Loss: 8402.8867  LR: 0.001000000\n",
      "Epoch: 380        Loss: 8402.0654  LR: 0.001000000\n",
      "Epoch: 381        Loss: 8401.2471  LR: 0.001000000\n",
      "Epoch: 382        Loss: 8400.4316  LR: 0.001000000\n",
      "Epoch: 383        Loss: 8399.6230  LR: 0.001000000\n",
      "Epoch: 384        Loss: 8398.8184  LR: 0.001000000\n",
      "Epoch: 385        Loss: 8398.0186  LR: 0.001000000\n",
      "Epoch: 386        Loss: 8397.2236  LR: 0.001000000\n",
      "Epoch: 387        Loss: 8396.4316  LR: 0.001000000\n",
      "Epoch: 388        Loss: 8395.6436  LR: 0.001000000\n",
      "Epoch: 389        Loss: 8394.8633  LR: 0.001000000\n",
      "Epoch: 390        Loss: 8394.0859  LR: 0.001000000\n",
      "Epoch: 391        Loss: 8393.3154  LR: 0.001000000\n",
      "Epoch: 392        Loss: 8392.5488  LR: 0.001000000\n",
      "Epoch: 393        Loss: 8391.7852  LR: 0.001000000\n",
      "Epoch: 394        Loss: 8391.0293  LR: 0.001000000\n",
      "Epoch: 395        Loss: 8390.2783  LR: 0.001000000\n",
      "Epoch: 396        Loss: 8389.5332  LR: 0.001000000\n",
      "Epoch: 397        Loss: 8388.7930  LR: 0.001000000\n",
      "Epoch: 398        Loss: 8388.0537  LR: 0.001000000\n",
      "Epoch: 399        Loss: 8387.3232  LR: 0.001000000\n",
      "Epoch: 400        Loss: 8386.6035  LR: 0.001000000\n",
      "Epoch: 401        Loss: 8385.8818  LR: 0.001000000\n",
      "Epoch: 402        Loss: 8385.1699  LR: 0.001000000\n",
      "Epoch: 403        Loss: 8384.4629  LR: 0.001000000\n",
      "Epoch: 404        Loss: 8383.8184  LR: 0.001000000\n",
      "Epoch: 405        Loss: 8383.1191  LR: 0.001000000\n",
      "Epoch: 406        Loss: 8382.4316  LR: 0.001000000\n",
      "Epoch: 407        Loss: 8381.7422  LR: 0.001000000\n",
      "Epoch: 408        Loss: 8381.0625  LR: 0.001000000\n",
      "Epoch: 409        Loss: 8380.3916  LR: 0.001000000\n",
      "Epoch: 410        Loss: 8379.7227  LR: 0.001000000\n",
      "Epoch: 411        Loss: 8379.0645  LR: 0.001000000\n",
      "Epoch: 412        Loss: 8378.4141  LR: 0.001000000\n",
      "Epoch: 413        Loss: 8377.7646  LR: 0.001000000\n",
      "Epoch: 414        Loss: 8377.1270  LR: 0.001000000\n",
      "Epoch: 415        Loss: 8376.4980  LR: 0.001000000\n",
      "Epoch: 416        Loss: 8375.8730  LR: 0.001000000\n",
      "Epoch: 417        Loss: 8375.2578  LR: 0.001000000\n",
      "Epoch: 418        Loss: 8374.6504  LR: 0.001000000\n",
      "Epoch: 419        Loss: 8374.0498  LR: 0.001000000\n",
      "Epoch: 420        Loss: 8373.4580  LR: 0.001000000\n",
      "Epoch: 421        Loss: 8372.8779  LR: 0.001000000\n",
      "Epoch: 422        Loss: 8372.3008  LR: 0.001000000\n",
      "Epoch: 423        Loss: 8371.7373  LR: 0.001000000\n",
      "Epoch: 424        Loss: 8371.1826  LR: 0.001000000\n",
      "Epoch: 425        Loss: 8370.6338  LR: 0.001000000\n",
      "Epoch: 426        Loss: 8370.1006  LR: 0.001000000\n",
      "Epoch: 427        Loss: 8369.5762  LR: 0.001000000\n",
      "Epoch: 428        Loss: 8369.0566  LR: 0.001000000\n",
      "Epoch: 429        Loss: 8368.5410  LR: 0.001000000\n",
      "Epoch: 430        Loss: 8368.0332  LR: 0.001000000\n",
      "Epoch: 431        Loss: 8367.5293  LR: 0.001000000\n",
      "Epoch: 432        Loss: 8367.0410  LR: 0.001000000\n",
      "Epoch: 433        Loss: 8366.5469  LR: 0.001000000\n",
      "Epoch: 434        Loss: 8366.0635  LR: 0.001000000\n",
      "Epoch: 435        Loss: 8365.5801  LR: 0.001000000\n",
      "Epoch: 436        Loss: 8365.1055  LR: 0.001000000\n",
      "Epoch: 437        Loss: 8364.6387  LR: 0.001000000\n",
      "Epoch: 438        Loss: 8364.1914  LR: 0.001000000\n",
      "Epoch: 439        Loss: 8363.7324  LR: 0.001000000\n",
      "Epoch: 440        Loss: 8363.4287  LR: 0.001000000\n",
      "Epoch: 441        Loss: 8362.9785  LR: 0.001000000\n",
      "Epoch: 442        Loss: 8362.5381  LR: 0.001000000\n",
      "Epoch: 443        Loss: 8362.1074  LR: 0.001000000\n",
      "Epoch: 444        Loss: 8361.6777  LR: 0.001000000\n",
      "Epoch: 445        Loss: 8361.5967  LR: 0.001000000\n",
      "Epoch: 446        Loss: 8361.2539  LR: 0.001000000\n",
      "Epoch: 447        Loss: 8361.1748  LR: 0.001000000\n",
      "Epoch: 448        Loss: 8360.8340  LR: 0.001000000\n",
      "Epoch: 449        Loss: 8360.7598  LR: 0.001000000\n",
      "Epoch: 450        Loss: 8360.4189  LR: 0.001000000\n",
      "Epoch: 451        Loss: 8360.3486  LR: 0.001000000\n",
      "Epoch: 452        Loss: 8360.0098  LR: 0.001000000\n",
      "Epoch: 453        Loss: 8359.9414  LR: 0.001000000\n",
      "Epoch: 454        Loss: 8359.6055  LR: 0.001000000\n",
      "Epoch: 455        Loss: 8359.5410  LR: 0.001000000\n",
      "Epoch: 456        Loss: 8359.2041  LR: 0.001000000\n",
      "Epoch: 457        Loss: 8359.1436  LR: 0.001000000\n",
      "Epoch: 458        Loss: 8358.8096  LR: 0.001000000\n",
      "Epoch: 459        Loss: 8358.7529  LR: 0.001000000\n",
      "Epoch: 460        Loss: 8358.4199  LR: 0.001000000\n",
      "Epoch: 461        Loss: 8358.3828  LR: 0.001000000\n",
      "Epoch: 462        Loss: 8358.0518  LR: 0.001000000\n",
      "Epoch: 463        Loss: 8358.0059  LR: 0.001000000\n",
      "Epoch: 464        Loss: 8357.6738  LR: 0.001000000\n",
      "Epoch: 465        Loss: 8357.6279  LR: 0.001000000\n",
      "Epoch: 466        Loss: 8357.2988  LR: 0.001000000\n",
      "Epoch: 467        Loss: 8357.2529  LR: 0.001000000\n",
      "Epoch: 468        Loss: 8356.9248  LR: 0.001000000\n",
      "Epoch: 469        Loss: 8356.8857  LR: 0.001000000\n",
      "Epoch: 470        Loss: 8356.5586  LR: 0.001000000\n",
      "Epoch: 471        Loss: 8356.5225  LR: 0.001000000\n",
      "Epoch: 472        Loss: 8356.1973  LR: 0.001000000\n",
      "Epoch: 473        Loss: 8356.1670  LR: 0.001000000\n",
      "Epoch: 474        Loss: 8355.8398  LR: 0.001000000\n",
      "Epoch: 475        Loss: 8355.8115  LR: 0.001000000\n",
      "Epoch: 476        Loss: 8355.4883  LR: 0.001000000\n",
      "Epoch: 477        Loss: 8355.4668  LR: 0.001000000\n",
      "Epoch: 478        Loss: 8355.1445  LR: 0.001000000\n",
      "Epoch: 479        Loss: 8355.1250  LR: 0.001000000\n",
      "Epoch: 480        Loss: 8354.8027  LR: 0.001000000\n",
      "Epoch: 481        Loss: 8354.7900  LR: 0.001000000\n",
      "Epoch: 482        Loss: 8354.4707  LR: 0.001000000\n",
      "Epoch: 483        Loss: 8354.4629  LR: 0.001000000\n",
      "Epoch: 484        Loss: 8354.1455  LR: 0.001000000\n",
      "Epoch: 485        Loss: 8354.1377  LR: 0.001000000\n",
      "Epoch: 486        Loss: 8353.8213  LR: 0.001000000\n",
      "Epoch: 487        Loss: 8353.8164  LR: 0.001000000\n",
      "Epoch: 488        Loss: 8353.5000  LR: 0.001000000\n",
      "Epoch: 489        Lr was reduced to: 0.000700000\n",
      "Epoch: 490        Loss: 8353.4512  LR: 0.000700000\n",
      "Epoch: 491        Loss: 8353.2812  LR: 0.000700000\n",
      "Epoch: 492        Loss: 8353.2334  LR: 0.000700000\n",
      "Epoch: 493        Loss: 8353.0635  LR: 0.000700000\n",
      "Epoch: 494        Loss: 8353.0176  LR: 0.000700000\n",
      "Epoch: 495        Loss: 8352.8496  LR: 0.000700000\n",
      "Epoch: 496        Loss: 8352.8057  LR: 0.000700000\n",
      "Epoch: 497        Loss: 8352.6377  LR: 0.000700000\n",
      "Epoch: 498        Loss: 8352.5977  LR: 0.000700000\n",
      "Epoch: 499        Loss: 8352.4297  LR: 0.000700000\n",
      "Epoch: 500        Loss: 8352.3916  LR: 0.000700000\n",
      "Epoch: 501        Loss: 8352.2236  LR: 0.000700000\n",
      "Epoch: 502        Loss: 8352.1875  LR: 0.000700000\n",
      "Epoch: 503        Loss: 8352.0225  LR: 0.000700000\n",
      "Epoch: 504        Loss: 8351.9883  LR: 0.000700000\n",
      "Epoch: 505        Loss: 8351.8213  LR: 0.000700000\n",
      "Epoch: 506        Loss: 8351.7930  LR: 0.000700000\n",
      "Epoch: 507        Loss: 8351.6279  LR: 0.000700000\n",
      "Epoch: 508        Loss: 8351.5986  LR: 0.000700000\n",
      "Epoch: 509        Loss: 8351.4346  LR: 0.000700000\n",
      "Epoch: 510        Loss: 8351.4072  LR: 0.000700000\n",
      "Epoch: 511        Loss: 8351.2432  LR: 0.000700000\n",
      "Epoch: 512        Loss: 8351.2188  LR: 0.000700000\n",
      "Epoch: 513        Loss: 8351.0547  LR: 0.000700000\n",
      "Epoch: 514        Loss: 8351.0312  LR: 0.000700000\n",
      "Epoch: 515        Loss: 8350.8691  LR: 0.000700000\n",
      "Epoch: 516        Loss: 8350.8506  LR: 0.000700000\n",
      "Epoch: 517        Loss: 8350.6875  LR: 0.000700000\n",
      "Epoch: 518        Loss: 8350.6699  LR: 0.000700000\n",
      "Epoch: 519        Loss: 8350.5078  LR: 0.000700000\n",
      "Epoch: 520        Loss: 8350.4941  LR: 0.000700000\n",
      "Epoch: 521        Loss: 8350.3340  LR: 0.000700000\n",
      "Epoch: 522        Loss: 8350.3203  LR: 0.000700000\n",
      "Epoch: 523        Loss: 8350.1602  LR: 0.000700000\n",
      "Epoch: 524        Loss: 8350.1543  LR: 0.000700000\n",
      "Epoch: 525        Loss: 8349.9951  LR: 0.000700000\n",
      "Epoch: 526        Loss: 8349.9863  LR: 0.000700000\n",
      "Epoch: 527        Loss: 8349.8281  LR: 0.000700000\n",
      "Epoch: 528        Loss: 8349.8232  LR: 0.000700000\n",
      "Epoch: 529        Loss: 8349.6641  LR: 0.000700000\n",
      "Epoch: 530        Loss: 8349.6611  LR: 0.000700000\n",
      "Epoch: 531        Loss: 8349.5039  LR: 0.000700000\n",
      "Epoch: 532        Loss did not change\n",
      "Epoch: 533        Loss: 8349.3467  LR: 0.000700000\n",
      "Epoch: 534        Lr was reduced to: 0.000490000\n",
      "Epoch: 535        Loss: 8349.3174  LR: 0.000490000\n",
      "Epoch: 536        Loss: 8349.2383  LR: 0.000490000\n",
      "Epoch: 537        Loss: 8349.2119  LR: 0.000490000\n",
      "Epoch: 538        Loss: 8349.1318  LR: 0.000490000\n",
      "Epoch: 539        Loss: 8349.1113  LR: 0.000490000\n",
      "Epoch: 540        Loss: 8349.0312  LR: 0.000490000\n",
      "Epoch: 541        Loss: 8349.0078  LR: 0.000490000\n",
      "Epoch: 542        Loss: 8348.9287  LR: 0.000490000\n",
      "Epoch: 543        Loss: 8348.9062  LR: 0.000490000\n",
      "Epoch: 544        Loss: 8348.8271  LR: 0.000490000\n",
      "Epoch: 545        Loss: 8348.8057  LR: 0.000490000\n",
      "Epoch: 546        Loss: 8348.7285  LR: 0.000490000\n",
      "Epoch: 547        Loss: 8348.7070  LR: 0.000490000\n",
      "Epoch: 548        Loss: 8348.6289  LR: 0.000490000\n",
      "Epoch: 549        Loss: 8348.6123  LR: 0.000490000\n",
      "Epoch: 550        Loss: 8348.5352  LR: 0.000490000\n",
      "Epoch: 551        Loss: 8348.5156  LR: 0.000490000\n",
      "Epoch: 552        Loss: 8348.4395  LR: 0.000490000\n",
      "Epoch: 553        Loss: 8348.4229  LR: 0.000490000\n",
      "Epoch: 554        Loss: 8348.3457  LR: 0.000490000\n",
      "Epoch: 555        Loss: 8348.3301  LR: 0.000490000\n",
      "Epoch: 556        Loss: 8348.2539  LR: 0.000490000\n",
      "Epoch: 557        Loss: 8348.2402  LR: 0.000490000\n",
      "Epoch: 558        Loss: 8348.1641  LR: 0.000490000\n",
      "Epoch: 559        Loss: 8348.1514  LR: 0.000490000\n",
      "Epoch: 560        Loss: 8348.0762  LR: 0.000490000\n",
      "Epoch: 561        Loss: 8348.0654  LR: 0.000490000\n",
      "Epoch: 562        Loss: 8347.9902  LR: 0.000490000\n",
      "Epoch: 563        Loss: 8347.9805  LR: 0.000490000\n",
      "Epoch: 564        Loss: 8347.9043  LR: 0.000490000\n",
      "Epoch: 565        Loss: 8347.8994  LR: 0.000490000\n",
      "Epoch: 566        Loss: 8347.8242  LR: 0.000490000\n",
      "Epoch: 567        Loss: 8347.8174  LR: 0.000490000\n",
      "Epoch: 568        Loss: 8347.7422  LR: 0.000490000\n",
      "Epoch: 569        Loss: 8347.7373  LR: 0.000490000\n",
      "Epoch: 570        Loss: 8347.6621  LR: 0.000490000\n",
      "Epoch: 571        Loss: 8347.6602  LR: 0.000490000\n",
      "Epoch: 572        Loss: 8347.5859  LR: 0.000490000\n",
      "Epoch: 573        Loss: 8347.5830  LR: 0.000490000\n",
      "Epoch: 574        Loss: 8347.5098  LR: 0.000490000\n",
      "Epoch: 575        Loss did not change\n",
      "Epoch: 576        Loss: 8347.4355  LR: 0.000490000\n",
      "Epoch: 577        Lr was reduced to: 0.000343000\n",
      "Epoch: 578        Loss: 8347.4180  LR: 0.000343000\n",
      "Epoch: 579        Loss: 8347.3848  LR: 0.000343000\n",
      "Epoch: 580        Loss: 8347.3682  LR: 0.000343000\n",
      "Epoch: 581        Loss: 8347.3359  LR: 0.000343000\n",
      "Epoch: 582        Loss: 8347.3193  LR: 0.000343000\n",
      "Epoch: 583        Loss: 8347.2861  LR: 0.000343000\n",
      "Epoch: 584        Loss: 8347.2715  LR: 0.000343000\n",
      "Epoch: 585        Loss: 8347.2373  LR: 0.000343000\n",
      "Epoch: 586        Loss: 8347.2236  LR: 0.000343000\n",
      "Epoch: 587        Loss: 8347.1904  LR: 0.000343000\n",
      "Epoch: 588        Loss: 8347.1787  LR: 0.000343000\n",
      "Epoch: 589        Loss: 8347.1455  LR: 0.000343000\n",
      "Epoch: 590        Loss: 8347.1328  LR: 0.000343000\n",
      "Epoch: 591        Loss: 8347.0996  LR: 0.000343000\n",
      "Epoch: 592        Loss: 8347.0869  LR: 0.000343000\n",
      "Epoch: 593        Loss: 8347.0547  LR: 0.000343000\n",
      "Epoch: 594        Loss: 8347.0430  LR: 0.000343000\n",
      "Epoch: 595        Loss: 8347.0107  LR: 0.000343000\n",
      "Epoch: 596        Loss: 8347.0000  LR: 0.000343000\n",
      "Epoch: 597        Loss: 8346.9678  LR: 0.000343000\n",
      "Epoch: 598        Loss: 8346.9570  LR: 0.000343000\n",
      "Epoch: 599        Loss: 8346.9248  LR: 0.000343000\n",
      "Epoch: 600        Loss: 8346.9160  LR: 0.000343000\n",
      "Epoch: 601        Loss: 8346.8838  LR: 0.000343000\n",
      "Epoch: 602        Loss: 8346.8740  LR: 0.000343000\n",
      "Epoch: 603        Loss: 8346.8428  LR: 0.000343000\n",
      "Epoch: 604        Loss: 8346.8379  LR: 0.000343000\n",
      "Epoch: 605        Loss: 8346.8057  LR: 0.000343000\n",
      "Epoch: 606        Loss: 8346.7979  LR: 0.000343000\n",
      "Epoch: 607        Loss: 8346.7656  LR: 0.000343000\n",
      "Epoch: 608        Loss: 8346.7598  LR: 0.000343000\n",
      "Epoch: 609        Loss: 8346.7275  LR: 0.000343000\n",
      "Epoch: 610        Loss: 8346.7217  LR: 0.000343000\n",
      "Epoch: 611        Loss: 8346.6895  LR: 0.000343000\n",
      "Epoch: 612        Loss: 8346.6855  LR: 0.000343000\n",
      "Epoch: 613        Loss: 8346.6533  LR: 0.000343000\n",
      "Epoch: 614        Loss: 8346.6484  LR: 0.000343000\n",
      "Epoch: 615        Loss: 8346.6182  LR: 0.000343000\n",
      "Epoch: 616        Loss: 8346.6143  LR: 0.000343000\n",
      "Epoch: 617        Loss: 8346.5820  LR: 0.000343000\n",
      "Epoch: 618        Loss: 8346.5791  LR: 0.000343000\n",
      "Epoch: 619        Loss: 8346.5479  LR: 0.000343000\n",
      "Epoch: 620        Loss: 8346.5469  LR: 0.000343000\n",
      "Epoch: 621        Loss: 8346.5156  LR: 0.000343000\n",
      "Epoch: 622        Loss did not change\n",
      "Epoch: 623        Loss: 8346.4834  LR: 0.000343000\n",
      "Epoch: 624        Loss did not change\n",
      "Epoch: 625        Loss: 8346.4512  LR: 0.000343000\n",
      "Epoch: 626        Lr was reduced to: 0.000240100\n",
      "Epoch: 627        Loss: 8346.4424  LR: 0.000240100\n",
      "Epoch: 628        Loss: 8346.4297  LR: 0.000240100\n",
      "Epoch: 629        Loss: 8346.4209  LR: 0.000240100\n",
      "Epoch: 630        Loss: 8346.4092  LR: 0.000240100\n",
      "Epoch: 631        Loss: 8346.3994  LR: 0.000240100\n",
      "Epoch: 632        Loss: 8346.3877  LR: 0.000240100\n",
      "Epoch: 633        Loss: 8346.3789  LR: 0.000240100\n",
      "Epoch: 634        Loss: 8346.3682  LR: 0.000240100\n",
      "Epoch: 635        Loss: 8346.3604  LR: 0.000240100\n",
      "Epoch: 636        Loss: 8346.3477  LR: 0.000240100\n",
      "Epoch: 637        Loss: 8346.3398  LR: 0.000240100\n",
      "Epoch: 638        Loss: 8346.3281  LR: 0.000240100\n",
      "Epoch: 639        Loss: 8346.3213  LR: 0.000240100\n",
      "Epoch: 640        Loss: 8346.3096  LR: 0.000240100\n",
      "Epoch: 641        Loss: 8346.3027  LR: 0.000240100\n",
      "Epoch: 642        Loss: 8346.2910  LR: 0.000240100\n",
      "Epoch: 643        Loss: 8346.2842  LR: 0.000240100\n",
      "Epoch: 644        Loss: 8346.2734  LR: 0.000240100\n",
      "Epoch: 645        Loss: 8346.2666  LR: 0.000240100\n",
      "Epoch: 646        Loss: 8346.2549  LR: 0.000240100\n",
      "Epoch: 647        Loss: 8346.2500  LR: 0.000240100\n",
      "Epoch: 648        Loss: 8346.2383  LR: 0.000240100\n",
      "Epoch: 649        Loss: 8346.2324  LR: 0.000240100\n",
      "Epoch: 650        Loss: 8346.2217  LR: 0.000240100\n",
      "Epoch: 651        Loss: 8346.2168  LR: 0.000240100\n",
      "Epoch: 652        Loss: 8346.2051  LR: 0.000240100\n",
      "Epoch: 653        Loss: 8346.2012  LR: 0.000240100\n",
      "Epoch: 654        Loss: 8346.1904  LR: 0.000240100\n",
      "Epoch: 655        Loss: 8346.1865  LR: 0.000240100\n",
      "Epoch: 656        Loss: 8346.1748  LR: 0.000240100\n",
      "Epoch: 657        Loss: 8346.1709  LR: 0.000240100\n",
      "Epoch: 658        Loss: 8346.1602  LR: 0.000240100\n",
      "Epoch: 659        Loss: 8346.1572  LR: 0.000240100\n",
      "Epoch: 660        Loss: 8346.1455  LR: 0.000240100\n",
      "Epoch: 661        Loss: 8346.1426  LR: 0.000240100\n",
      "Epoch: 662        Loss: 8346.1318  LR: 0.000240100\n",
      "Epoch: 663        Loss: 8346.1289  LR: 0.000240100\n",
      "Epoch: 664        Loss: 8346.1182  LR: 0.000240100\n",
      "Epoch: 665        Loss: 8346.1162  LR: 0.000240100\n",
      "Epoch: 666        Loss: 8346.1064  LR: 0.000240100\n",
      "Epoch: 667        Loss: 8346.1035  LR: 0.000240100\n",
      "Epoch: 668        Loss: 8346.0918  LR: 0.000240100\n",
      "Epoch: 669        Loss did not change\n",
      "Epoch: 670        Loss: 8346.0801  LR: 0.000240100\n",
      "Epoch: 671        Loss did not change\n",
      "Epoch: 672        Loss: 8346.0693  LR: 0.000240100\n",
      "Epoch: 673        Loss did not change\n",
      "Epoch: 674        Loss: 8346.0586  LR: 0.000240100\n",
      "Epoch: 675        Loss did not change\n",
      "Epoch: 676        Loss: 8346.0479  LR: 0.000240100\n",
      "Epoch: 677        Loss did not change\n",
      "Epoch: 678        Loss: 8346.0371  LR: 0.000240100\n",
      "Epoch: 679        Lr was reduced to: 0.000168070\n",
      "Epoch: 680        Loss: 8346.0332  LR: 0.000168070\n",
      "Epoch: 681        Loss: 8346.0303  LR: 0.000168070\n",
      "Epoch: 682        Loss: 8346.0264  LR: 0.000168070\n",
      "Epoch: 683        Loss: 8346.0244  LR: 0.000168070\n",
      "Epoch: 684        Loss: 8346.0195  LR: 0.000168070\n",
      "Epoch: 685        Loss: 8346.0176  LR: 0.000168070\n",
      "Epoch: 686        Loss: 8346.0137  LR: 0.000168070\n",
      "Epoch: 687        Loss: 8346.0117  LR: 0.000168070\n",
      "Epoch: 688        Loss: 8346.0088  LR: 0.000168070\n",
      "Epoch: 689        Loss: 8346.0059  LR: 0.000168070\n",
      "Epoch: 690        Loss: 8346.0020  LR: 0.000168070\n",
      "Epoch: 691        Loss: 8346.0000  LR: 0.000168070\n",
      "Epoch: 692        Loss: 8345.9971  LR: 0.000168070\n",
      "Epoch: 693        Loss: 8345.9951  LR: 0.000168070\n",
      "Epoch: 694        Loss: 8345.9922  LR: 0.000168070\n",
      "Epoch: 695        Loss: 8345.9893  LR: 0.000168070\n",
      "Epoch: 696        Loss: 8345.9873  LR: 0.000168070\n",
      "Epoch: 697        Loss: 8345.9844  LR: 0.000168070\n",
      "Epoch: 698        Loss: 8345.9814  LR: 0.000168070\n",
      "Epoch: 699        Loss: 8345.9805  LR: 0.000168070\n",
      "Epoch: 700        Loss: 8345.9766  LR: 0.000168070\n",
      "Epoch: 701        Loss: 8345.9756  LR: 0.000168070\n",
      "Epoch: 702        Loss: 8345.9727  LR: 0.000168070\n",
      "Epoch: 703        Loss: 8345.9717  LR: 0.000168070\n",
      "Epoch: 704        Loss: 8345.9697  LR: 0.000168070\n",
      "Epoch: 705        Loss: 8345.9678  LR: 0.000168070\n",
      "Epoch: 706        Loss: 8345.9658  LR: 0.000168070\n",
      "Epoch: 707        Loss: 8345.9639  LR: 0.000168070\n",
      "Epoch: 708        Loss: 8345.9629  LR: 0.000168070\n",
      "Epoch: 709        Loss: 8345.9600  LR: 0.000168070\n",
      "Epoch: 710        Loss did not change\n",
      "Epoch: 711        Loss: 8345.9570  LR: 0.000168070\n",
      "Epoch: 712        Loss: 8345.9561  LR: 0.000168070\n",
      "Epoch: 713        Loss: 8345.9551  LR: 0.000168070\n",
      "Epoch: 714        Loss: 8345.9531  LR: 0.000168070\n",
      "Epoch: 715        Loss: 8345.9512  LR: 0.000168070\n",
      "Epoch: 716        Loss did not change\n",
      "Epoch: 717        Loss: 8345.9482  LR: 0.000168070\n",
      "Epoch: 718        Lr was reduced to: 0.000117649\n",
      "Epoch: 719        Loss: 8345.9463  LR: 0.000117649\n",
      "Epoch: 720        Lr was reduced to: 0.000082354\n",
      "Epoch: 721        Loss did not change\n",
      "Epoch: 722        Loss: 8345.9443  LR: 0.000082354\n",
      "Epoch: 723        Loss did not change\n",
      "Epoch: 724        Loss: 8345.9434  LR: 0.000082354\n",
      "Epoch: 725        Loss did not change\n",
      "Epoch: 726        Loss did not change\n",
      "Epoch: 727        Loss: 8345.9424  LR: 0.000082354\n",
      "Epoch: 728        Loss did not change\n",
      "Epoch: 729        Loss: 8345.9414  LR: 0.000082354\n",
      "Epoch: 730        Lr was reduced to: 0.000057648\n",
      "Epoch: 731        Loss: 8345.9404  LR: 0.000057648\n",
      "Epoch: 732        Loss: 8345.9395  LR: 0.000057648\n",
      "Epoch: 733        Loss did not change\n",
      "Epoch: 734        Loss did not change\n",
      "Epoch: 735        Loss did not change\n",
      "Epoch: 736        Loss did not change\n",
      "Epoch: 737        Loss did not change\n",
      "Epoch: 738        Loss did not change\n",
      "Epoch: 739        Loss: 8345.9385  LR: 0.000057648\n",
      "Epoch: 740        Loss did not change\n",
      "Epoch: 741        Loss did not change\n",
      "Epoch: 742        Loss: 8345.9375  LR: 0.000057648\n",
      "Epoch: 743        Loss did not change\n",
      "Epoch: 744        Loss did not change\n",
      "Epoch: 745        Loss did not change\n",
      "Epoch: 746        Loss: 8345.9365  LR: 0.000057648\n",
      "Epoch: 747        Lr was reduced to: 0.000040354\n",
      "Epoch: 748        Loss did not change\n",
      "Epoch: 749        Loss did not change\n",
      "Epoch: 750        Loss: 8345.9355  LR: 0.000040354\n",
      "Epoch: 751        Lr was reduced to: 0.000028248\n",
      "Epoch: 752        Lr was reduced to: 0.000019773\n",
      "Epoch: 753        Loss did not change\n",
      "Epoch: 754        Loss did not change\n",
      "Epoch: 755        Loss did not change\n",
      "Epoch: 756        Loss did not change\n",
      "Epoch: 757        Loss did not change\n",
      "Epoch: 758        Loss did not change\n",
      "Epoch: 759        Loss did not change\n",
      "Epoch: 760        Loss did not change\n",
      "Epoch: 761        Loss did not change\n",
      "Epoch: 762        Loss did not change\n",
      "Epoch: 763        Loss: 8345.9346  LR: 0.000019773\n",
      "Epoch: 764        Loss did not change\n",
      "Epoch: 765        Loss: 8345.9336  LR: 0.000019773\n",
      "Epoch: 766        Lr was reduced to: 0.000013841\n",
      "Epoch: 767        Lr was reduced to: 0.000009689\n",
      "Epoch: 768        Lr was reduced to: 0.000006782\n",
      "Epoch: 769        Lr was reduced to: 0.000004748\n",
      "Epoch: 770        Lr was reduced to: 0.000003323\n",
      "Epoch: 771        Lr was reduced to: 0.000002326\n",
      "Epoch: 772        Lr was reduced to: 0.000001628\n",
      "Epoch: 773        Lr was reduced to: 0.000001140\n",
      "Epoch: 774        Lr was reduced to: 0.000000798\n",
      "The step size is too small: 7.979226629761193e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "measure = pm.Measure(locations=l, weights=w, device=dev)\n",
    "opt = pm.Optimizer([measure], lr=1e-3)\n",
    "opt.minimize(KDENLLLoss, verbose=True, print_freq=1, max_epochs=1000, tol_const=1e-2)\n",
    "\n",
    "plt.plot()\n",
    "\n",
    "mu=0 #Create true values\n",
    "sigma=1\n",
    "xs = l.detach()\n",
    "y=1/(np.sqrt(2*np.pi)*sigma)*torch.exp(-(xs+2-mu)**2/(2*sigma**2))\n",
    "y/=sum(y) #Normalize\n",
    "\n",
    "\n",
    "measure.visualize()\n",
    "plt.bar(l-0.1, torch.sum(kde_mat, dim=0)/torch.sum(kde_mat), zorder=0, width=0.1)\n",
    "plt.scatter(xs, y, zorder=2)\n",
    "plt.legend(['KDE', 'PDF of true distribution','Measure'])\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\OneDrive - Chalmers\\r 3\\Kandidatarbete\\Python Git\\Sergei\\torch_measure.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.locations = torch.tensor(locations, dtype= torch.float)\n",
      "D:\\OneDrive - Chalmers\\r 3\\Kandidatarbete\\Python Git\\Sergei\\torch_measure.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.weights = nn.parameter.Parameter(torch.tensor(weights, dtype= torch.float))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'TorchMeasure' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[115], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mSergei\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtorch_measure\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01ms\u001B[39;00m\n\u001B[0;32m      3\u001B[0m mes \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mTorchMeasure(l, w)\n\u001B[1;32m----> 4\u001B[0m opt \u001B[38;5;241m=\u001B[39m \u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMeasureMinimizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mKDENLLLoss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m opt\u001B[38;5;241m.\u001B[39mminimize()\n\u001B[0;32m      8\u001B[0m plt\u001B[38;5;241m.\u001B[39mscatter(xs, y, zorder\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32mD:\\OneDrive - Chalmers\\r 3\\Kandidatarbete\\Python Git\\Sergei\\torch_measure.py:215\u001B[0m, in \u001B[0;36mMeasureMinimizer.__init__\u001B[1;34m(self, mes, goal_fn, learning_rate, **goal_func_kwargs)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgoal_func_kwargs \u001B[38;5;241m=\u001B[39m goal_func_kwargs\n\u001B[0;32m    213\u001B[0m \u001B[38;5;66;03m# self.grad = None\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;66;03m# computed initial values:\u001B[39;00m\n\u001B[1;32m--> 215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgoal_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmes, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgoal_func_kwargs)\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmes\u001B[38;5;241m.\u001B[39mweights\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;66;03m# clear the gradients\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval\u001B[38;5;241m.\u001B[39mbackward() \u001B[38;5;66;03m# compute the gradient\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[113], line 40\u001B[0m, in \u001B[0;36mKDENLLLoss\u001B[1;34m(m)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mKDENLLLoss\u001B[39m(m):\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m(torch\u001B[38;5;241m.\u001B[39mmatmul(kde_mat ,\u001B[43mm\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mweights\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m/\u001B[39m(M\u001B[38;5;241m*\u001B[39mh))\u001B[38;5;241m.\u001B[39mlog()\u001B[38;5;241m.\u001B[39msum()\n",
      "\u001B[1;31mTypeError\u001B[0m: 'TorchMeasure' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import Sergei.torch_measure as s\n",
    "\n",
    "mes = s.TorchMeasure(l, w)\n",
    "opt = s.MeasureMinimizer(mes, KDENLLLoss, learning_rate=1e-1)\n",
    "opt.minimize()\n",
    "\n",
    "\n",
    "plt.scatter(xs, y, zorder=2)\n",
    "plt.bar(l, mes.weights.detach())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
